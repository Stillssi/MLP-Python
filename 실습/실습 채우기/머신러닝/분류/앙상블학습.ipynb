{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀와 KNN을 기반으로 소프트 보팅 방식으로 새롭게 보팅 분류기 만듦\n",
    "- VotingClassifier 클래스 : 보팅 분류기 생성\n",
    "- 매개변수: estimators(리스트 값으로 보팅에 사용될 여러개의 Classifier 객체들을 튜플형식으로)와 voting('hard'시 하드보팅, 'soft'시 소프트 보팅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#개별 모델은 로지스틱 회귀와 KNN\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "#개별 모델을 소프트 보팅을 기반의 앙상블 모델로 구현한 분류기\n",
    "vo_clf = VotingClassifier(estimators=[('LR',lr_clf),('KNN',knn_clf)], voting='soft')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=156)\n",
    "\n",
    "#학습, 예측, 평가\n",
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "print('Voting 분류기 정확도: {0: .4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개별 모델의 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name=classifier.__class__.__name__\n",
    "    print('{0}정확도: {1: .4f}'.format(class_name, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤포레스트(배깅에 속함!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name']=new_feature_name_df[['column_name','dup_cnt']].apply(lambda x:x[0]+'_'+str(x[1]) if x[1]>0 else x[0],axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'],axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_dataset():\n",
    "    feature_name_df = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/features.txt', sep='\\s+', header=None, names=['column_index','column_name'])\n",
    "    #중복된 피처명을 수정하는 get_new_feature_name_df()을 이용, 신규 피처명 Dataframe 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    #DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 반환\n",
    "    feature_name = new_feature_name_df.iloc[:,1].values.tolist()\n",
    "    #학습 피처 데이터 세트와 테스트 피처 데이터를 DataFrame으로 로딩, 칼럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/train/X_train.txt', sep='\\s+', header=None, names=feature_name)\n",
    "    X_test = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/test/X_test.txt', sep='\\s+', header=None, names=feature_name)\n",
    "\n",
    "    y_train = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n",
    "    y_test = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n",
    "\n",
    "    return X_train,X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "#랜덤 포스 학습 및 별도의 테트 세트 예측성능 평가\n",
    "rf_clf = RandomForestClassifier(random_state=0, max_depth=8)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "accuracy= accuracy_score(y_test, pred)\n",
    "print('랜덤 포레스트 정확도: {0: .4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': [8,16,24],\n",
    "    'min_samples_leaf':[1,6,12],\n",
    "    'min_samples_split':[2,8,16]\n",
    "}\n",
    "\n",
    "#RandomForestClassifier 객체 생성 후 GridSearchCV 수행\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs=-1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0: .4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적의 파라미터로 학습, 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf1 = RandomForestClassifier(n_estimators=100, min_samples_leaf=6, max_depth=16, min_samples_split=2, random_state=0)\n",
    "rf_clf1.fit(X_train, y_train)\n",
    "rf_clf1.predict(X_test)\n",
    "print('예측 정확도: {0: .4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ftr_importances_values = rf_clf1.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature importances Top 20')\n",
    "sns.barplot(x=ftr_top20, y=ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM(부스팅 알고리즘) 기반 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "#GBM 수행시간 측정을 위함, 시작시간 설정\n",
    "start_time = time.time()\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print('GBM 정확도: {0: .4f}'.format(gb_accuracy))\n",
    "print('GBM 수행 시간: {0: .1f} 초'.format(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter.ttk import LabeledScale\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "features = dataset.data\n",
    "labels = dataset.target\n",
    "\n",
    "cancer_df = pd.DataFrame(data=features, columns=dataset.feature_names)\n",
    "cancer_df['target']=labels\n",
    "cancer_df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.target_names)\n",
    "print(cancer_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = cancer_df.iloc[:, :-1]\n",
    "y_label = cancer_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=156)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(X_tr.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = xgb.DMatrix(data=X_tr, label=y_tr)\n",
    "dval = xgb.DMatrix(data=X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth':3,\n",
    "    'eta':0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "num_rounds= 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = [(dtr, 'train'), (dval, 'eval')]\n",
    "xgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round = num_rounds, early_stopping_rounds=50, evals=eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs=xgb_model.predict(dtest)\n",
    "print('predict() 수행 결과값을 10개만 표시, 예측 확률값으로 표시됨')\n",
    "print(np.round(pred_probs[:10],3))\n",
    "\n",
    "preds=[1 if x>0.5 else 0 for x in pred_probs]\n",
    "print('예측값 10개만 표시', preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차행렬')\n",
    "    print(confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도:{1: .4f}, 재현율:{2: .4f}, f1점수: {3: .4f}, AUC점수:{4: .4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, preds, pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런 래퍼 XGBClassifier, XGBRegressor\n",
    "- 매개변수: eta -> learning_rate, sub_sample->subsample, lambda->reg_lambda, alpha->reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3, eval_metric='logloss')\n",
    "xgb_wrapper.fit(X_train, y_train, verbose=True)\n",
    "w_preds = xgb_wrapper.predict(X_test)\n",
    "w_pred_proba=xgb_wrapper.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, w_preds, w_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조기중단 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "from xgboost import XGBClassifier\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3)\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "ws50_preds = xgb_wrapper.predict(X_test)\n",
    "ws50_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, ws50_preds, ws50_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조기종료 라운드 수 10으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=10, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "ws10_pred = xgb_wrapper.predict(X_test)\n",
    "ws10_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test, ws10_pred, ws10_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "cancer_df['target'] = dataset.target\n",
    "X_feature = cancer_df.iloc[:,:-1]\n",
    "y_label = cancer_df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feature, y_label, test_size=0.2, random_state=156)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=156)\n",
    "\n",
    "lgbm_wrapper = LGBMClassifier(n_estimators=400, learning_rate=0.05)\n",
    "\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "lgbm_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "preds = lgbm_wrapper.predict(X_test)\n",
    "pred_proba = lgbm_wrapper.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,12))\n",
    "plot_importance(lgbm_wrapper, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 베이지안 최적화 기반의 HyperOpt를 이용한 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth' : [10,20,30,40,50], \n",
    "    'num_leaves' : [35,45,55,65],\n",
    "    'colsample_bytree':[0.5, 0.6,0.7,0.8,0.9], \n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'min_child_weight': [10,20,30,40],\n",
    "    'reg_alpha': [0.01, 0.05, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류 실습 - 캐들 산탄데르 고객 만족 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "cust_df = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/train.csv', encoding='latin1')\n",
    "print('dataset shape', cust_df.shape)\n",
    "cust_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cust_df['TARGET'].value_counts())\n",
    "unsatisfied_cnt = cust_df[cust_df['TARGET']==1].TARGET.count()\n",
    "total_cnt = cust_df.TARGET.count()\n",
    "print('unsatisfied 비율은 {0: .2f}'.format(unsatisfied_cnt/total_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var3의 평균값 -999999 -> NaN이나 특정 예외값을 -999999fh 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cust_df.var3.value_counts()[:10]) #-999999값이 116개 있음 -> 다른 값에 비해 편차가 심하므로 2로 뱐환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df['var3'].replace(-999999, 2, inplace=True)\n",
    "#cust_df.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "#피처 세트와 레이블 분리, 레이블 칼럼은 DataFrame 마지막에 위치해 -1로 분리\n",
    "X_feature = cust_df.iloc[:,:-1]\n",
    "y_labels = cust_df.iloc[:, -1]\n",
    "print('피처 데이터 shape:{0}'.format(X_feature.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feature, y_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "\n",
    "print('학습 세트 Shape: {0}, 테스트 세트 shape:{1}'.format(X_train.shape, X_test.shape))\n",
    "print('학습 세트 레이블 값 분포 비율')\n",
    "print(y_train.value_counts()/train_cnt)\n",
    "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
    "print(y_test.value_counts()/test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost 조기 중단의 검증 데이터 세트로 사용하기 위해 학습 데이터 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#n_estimator는 500으로, random_state는 예제 수행 시마다 동일 예측 결과를 위해 수정\n",
    "xgb_clf = XGBClassifier(n_estimator=500, learning_rate=0.05, random_state=156)\n",
    "\n",
    "#성능 평가 지표는 auc로 조기 중간 파라밈터는 100으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=[(X_tr, y_tr), (X_val, y_val)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC: {0: .4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "#max_depth는 5에서 15까지 1간격으로, min_child_weight는 1~6까지 1간격으로\n",
    "#colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01dptj 0.2사이 정규 분포된 값으로 검색\n",
    "#quniform : 간격설정, uniform: 사이\n",
    "xgb_search_space = {\n",
    "    'max_depth': hp.quniform('max_depth',5,15,1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1,6,1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 2)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective_func(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimator=100, \n",
    "    max_depth=int(search_space['max_depth']), \n",
    "    min_child_weight=int(search_space['min_child_weight']), \n",
    "    colsample_bytree=search_space['colsample_bytree'], \n",
    "    learning_rate=search_space['learning_rate'])\n",
    "\n",
    "    #3개의 k-fold 방식으로 평가된 roc-auc 지표를 담는 리스트\n",
    "    roc_auc_list = []\n",
    "\n",
    "    #3개의 k-fold방식 적용\n",
    "    kf = KFold(n_splits=3)\n",
    "    #X_train을 다시 학습과 검증 데이터로 분리\n",
    "    for tr_index, val_index in kf.split(X_train):\n",
    "        #kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 분리\n",
    "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
    "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=30, eval_metric='auc', eval_set=[(X_tr, y_tr), (X_val, y_val)])\n",
    "\n",
    "        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:,1])\n",
    "        roc_auc_list.append(score)\n",
    "\n",
    "    #3개의 k-fold로 계산된 roc_auc값의 평균값을 반환하되\n",
    "    #HyperOpt는 목적 함수의 최소값을 위한 입력값을 찾음으로 -1을 곱한 뒤 반환\n",
    "    return -1*np.mean(roc_auc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "import numpy as np\n",
    "trials = Trials()\n",
    "xgb_search_space = {\n",
    "    'max_depth': hp.quniform('max_depth',5,15,1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1,6,1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 2)\n",
    "}\n",
    "\n",
    "#fmin()함수 호출, max_evals지정된 횟수만큼 반복 후 목적 함수 최소값을 가지는 최적 입력값 추출\n",
    "best = fmin(fn=objective_func, space=xgb_search_space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=np.random.default_rng(seed=30))\n",
    "\n",
    "# print('best:',best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators를 500 증가 후 최적으로 찾은 하이퍼 파라미터를 기반으로 학습과 예측 수행\n",
    "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'],5), max_depth=int(best['max_depth']),\n",
    "min_child_weight=int(best['min_child_weight']), colsample_bytree=round(best['colsample_bytree'],15))\n",
    "\n",
    "#evaluation metric을 auc로, early stopping은 100으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric='auc', eval_set=[(X_tr, y_tr),(X_val, y_val)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC 점수: {0: .4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax = plt.subplot(1,1,figsize=(10,8))\n",
    "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LightGBM 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's auc: 0.82625\ttraining's binary_logloss: 0.15523\tvalid_1's auc: 0.809814\tvalid_1's binary_logloss: 0.15774\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\ttraining's auc: 0.83366\ttraining's binary_logloss: 0.149566\tvalid_1's auc: 0.812647\tvalid_1's binary_logloss: 0.153249\n",
      "[3]\ttraining's auc: 0.839786\ttraining's binary_logloss: 0.145331\tvalid_1's auc: 0.814983\tvalid_1's binary_logloss: 0.150043\n",
      "[4]\ttraining's auc: 0.84588\ttraining's binary_logloss: 0.142002\tvalid_1's auc: 0.820013\tvalid_1's binary_logloss: 0.147504\n",
      "[5]\ttraining's auc: 0.848189\ttraining's binary_logloss: 0.139394\tvalid_1's auc: 0.821242\tvalid_1's binary_logloss: 0.145447\n",
      "[6]\ttraining's auc: 0.853423\ttraining's binary_logloss: 0.137158\tvalid_1's auc: 0.820464\tvalid_1's binary_logloss: 0.143963\n",
      "[7]\ttraining's auc: 0.85535\ttraining's binary_logloss: 0.135295\tvalid_1's auc: 0.821721\tvalid_1's binary_logloss: 0.142658\n",
      "[8]\ttraining's auc: 0.85958\ttraining's binary_logloss: 0.133521\tvalid_1's auc: 0.824205\tvalid_1's binary_logloss: 0.141567\n",
      "[9]\ttraining's auc: 0.86306\ttraining's binary_logloss: 0.132033\tvalid_1's auc: 0.825462\tvalid_1's binary_logloss: 0.140592\n",
      "[10]\ttraining's auc: 0.864775\ttraining's binary_logloss: 0.130692\tvalid_1's auc: 0.825941\tvalid_1's binary_logloss: 0.139767\n",
      "[11]\ttraining's auc: 0.867383\ttraining's binary_logloss: 0.129468\tvalid_1's auc: 0.826985\tvalid_1's binary_logloss: 0.13903\n",
      "[12]\ttraining's auc: 0.871302\ttraining's binary_logloss: 0.128294\tvalid_1's auc: 0.827315\tvalid_1's binary_logloss: 0.138544\n",
      "[13]\ttraining's auc: 0.873541\ttraining's binary_logloss: 0.127266\tvalid_1's auc: 0.8268\tvalid_1's binary_logloss: 0.138217\n",
      "[14]\ttraining's auc: 0.875669\ttraining's binary_logloss: 0.126321\tvalid_1's auc: 0.82663\tvalid_1's binary_logloss: 0.137868\n",
      "[15]\ttraining's auc: 0.878188\ttraining's binary_logloss: 0.125388\tvalid_1's auc: 0.827745\tvalid_1's binary_logloss: 0.137536\n",
      "[16]\ttraining's auc: 0.879387\ttraining's binary_logloss: 0.124547\tvalid_1's auc: 0.827891\tvalid_1's binary_logloss: 0.137193\n",
      "[17]\ttraining's auc: 0.88069\ttraining's binary_logloss: 0.123806\tvalid_1's auc: 0.828075\tvalid_1's binary_logloss: 0.136968\n",
      "[18]\ttraining's auc: 0.88162\ttraining's binary_logloss: 0.12313\tvalid_1's auc: 0.829014\tvalid_1's binary_logloss: 0.136726\n",
      "[19]\ttraining's auc: 0.883329\ttraining's binary_logloss: 0.122475\tvalid_1's auc: 0.829054\tvalid_1's binary_logloss: 0.136518\n",
      "[20]\ttraining's auc: 0.885515\ttraining's binary_logloss: 0.12171\tvalid_1's auc: 0.828658\tvalid_1's binary_logloss: 0.136438\n",
      "[21]\ttraining's auc: 0.886654\ttraining's binary_logloss: 0.121068\tvalid_1's auc: 0.828748\tvalid_1's binary_logloss: 0.136331\n",
      "[22]\ttraining's auc: 0.887663\ttraining's binary_logloss: 0.120466\tvalid_1's auc: 0.828813\tvalid_1's binary_logloss: 0.136318\n",
      "[23]\ttraining's auc: 0.888889\ttraining's binary_logloss: 0.1199\tvalid_1's auc: 0.828405\tvalid_1's binary_logloss: 0.136215\n",
      "[24]\ttraining's auc: 0.890306\ttraining's binary_logloss: 0.11935\tvalid_1's auc: 0.829523\tvalid_1's binary_logloss: 0.136047\n",
      "[25]\ttraining's auc: 0.891682\ttraining's binary_logloss: 0.118856\tvalid_1's auc: 0.829411\tvalid_1's binary_logloss: 0.135996\n",
      "[26]\ttraining's auc: 0.893428\ttraining's binary_logloss: 0.118384\tvalid_1's auc: 0.830649\tvalid_1's binary_logloss: 0.135789\n",
      "[27]\ttraining's auc: 0.894478\ttraining's binary_logloss: 0.117883\tvalid_1's auc: 0.830572\tvalid_1's binary_logloss: 0.135762\n",
      "[28]\ttraining's auc: 0.896292\ttraining's binary_logloss: 0.117424\tvalid_1's auc: 0.830564\tvalid_1's binary_logloss: 0.135727\n",
      "[29]\ttraining's auc: 0.89825\ttraining's binary_logloss: 0.116942\tvalid_1's auc: 0.830774\tvalid_1's binary_logloss: 0.135641\n",
      "[30]\ttraining's auc: 0.899901\ttraining's binary_logloss: 0.116463\tvalid_1's auc: 0.830975\tvalid_1's binary_logloss: 0.135595\n",
      "[31]\ttraining's auc: 0.900845\ttraining's binary_logloss: 0.116022\tvalid_1's auc: 0.831192\tvalid_1's binary_logloss: 0.135586\n",
      "[32]\ttraining's auc: 0.902055\ttraining's binary_logloss: 0.115612\tvalid_1's auc: 0.830941\tvalid_1's binary_logloss: 0.135586\n",
      "[33]\ttraining's auc: 0.904144\ttraining's binary_logloss: 0.115211\tvalid_1's auc: 0.831172\tvalid_1's binary_logloss: 0.135531\n",
      "[34]\ttraining's auc: 0.906379\ttraining's binary_logloss: 0.114692\tvalid_1's auc: 0.830955\tvalid_1's binary_logloss: 0.135563\n",
      "[35]\ttraining's auc: 0.907373\ttraining's binary_logloss: 0.114302\tvalid_1's auc: 0.831041\tvalid_1's binary_logloss: 0.135566\n",
      "[36]\ttraining's auc: 0.908273\ttraining's binary_logloss: 0.11397\tvalid_1's auc: 0.83098\tvalid_1's binary_logloss: 0.135602\n",
      "[37]\ttraining's auc: 0.909597\ttraining's binary_logloss: 0.113462\tvalid_1's auc: 0.830948\tvalid_1's binary_logloss: 0.135588\n",
      "[38]\ttraining's auc: 0.910354\ttraining's binary_logloss: 0.11307\tvalid_1's auc: 0.830998\tvalid_1's binary_logloss: 0.135574\n",
      "[39]\ttraining's auc: 0.912125\ttraining's binary_logloss: 0.112535\tvalid_1's auc: 0.831229\tvalid_1's binary_logloss: 0.135562\n",
      "[40]\ttraining's auc: 0.912768\ttraining's binary_logloss: 0.112205\tvalid_1's auc: 0.831266\tvalid_1's binary_logloss: 0.135538\n",
      "[41]\ttraining's auc: 0.913483\ttraining's binary_logloss: 0.111894\tvalid_1's auc: 0.831309\tvalid_1's binary_logloss: 0.135506\n",
      "[42]\ttraining's auc: 0.914751\ttraining's binary_logloss: 0.111472\tvalid_1's auc: 0.831334\tvalid_1's binary_logloss: 0.135504\n",
      "[43]\ttraining's auc: 0.915277\ttraining's binary_logloss: 0.111191\tvalid_1's auc: 0.831422\tvalid_1's binary_logloss: 0.135523\n",
      "[44]\ttraining's auc: 0.916186\ttraining's binary_logloss: 0.11081\tvalid_1's auc: 0.831125\tvalid_1's binary_logloss: 0.135605\n",
      "[45]\ttraining's auc: 0.917299\ttraining's binary_logloss: 0.110484\tvalid_1's auc: 0.831129\tvalid_1's binary_logloss: 0.135616\n",
      "[46]\ttraining's auc: 0.91807\ttraining's binary_logloss: 0.1101\tvalid_1's auc: 0.8314\tvalid_1's binary_logloss: 0.135591\n",
      "[47]\ttraining's auc: 0.91911\ttraining's binary_logloss: 0.10974\tvalid_1's auc: 0.831197\tvalid_1's binary_logloss: 0.135626\n",
      "[48]\ttraining's auc: 0.919725\ttraining's binary_logloss: 0.109449\tvalid_1's auc: 0.830986\tvalid_1's binary_logloss: 0.135664\n",
      "[49]\ttraining's auc: 0.920443\ttraining's binary_logloss: 0.109076\tvalid_1's auc: 0.831112\tvalid_1's binary_logloss: 0.135639\n",
      "[50]\ttraining's auc: 0.9215\ttraining's binary_logloss: 0.108758\tvalid_1's auc: 0.831208\tvalid_1's binary_logloss: 0.135639\n",
      "[51]\ttraining's auc: 0.922183\ttraining's binary_logloss: 0.108435\tvalid_1's auc: 0.831221\tvalid_1's binary_logloss: 0.135637\n",
      "[52]\ttraining's auc: 0.922497\ttraining's binary_logloss: 0.108188\tvalid_1's auc: 0.831354\tvalid_1's binary_logloss: 0.135584\n",
      "[53]\ttraining's auc: 0.923439\ttraining's binary_logloss: 0.107807\tvalid_1's auc: 0.830928\tvalid_1's binary_logloss: 0.135657\n",
      "[54]\ttraining's auc: 0.924483\ttraining's binary_logloss: 0.107429\tvalid_1's auc: 0.83098\tvalid_1's binary_logloss: 0.135684\n",
      "[55]\ttraining's auc: 0.924788\ttraining's binary_logloss: 0.10723\tvalid_1's auc: 0.831002\tvalid_1's binary_logloss: 0.135663\n",
      "[56]\ttraining's auc: 0.92516\ttraining's binary_logloss: 0.107014\tvalid_1's auc: 0.831207\tvalid_1's binary_logloss: 0.135629\n",
      "[57]\ttraining's auc: 0.925922\ttraining's binary_logloss: 0.106749\tvalid_1's auc: 0.831175\tvalid_1's binary_logloss: 0.135631\n",
      "[58]\ttraining's auc: 0.926195\ttraining's binary_logloss: 0.106528\tvalid_1's auc: 0.831092\tvalid_1's binary_logloss: 0.135658\n",
      "[59]\ttraining's auc: 0.926695\ttraining's binary_logloss: 0.106262\tvalid_1's auc: 0.831167\tvalid_1's binary_logloss: 0.135632\n",
      "[60]\ttraining's auc: 0.927196\ttraining's binary_logloss: 0.10604\tvalid_1's auc: 0.831251\tvalid_1's binary_logloss: 0.135611\n",
      "[61]\ttraining's auc: 0.927762\ttraining's binary_logloss: 0.105739\tvalid_1's auc: 0.831222\tvalid_1's binary_logloss: 0.135682\n",
      "[62]\ttraining's auc: 0.92812\ttraining's binary_logloss: 0.105538\tvalid_1's auc: 0.831193\tvalid_1's binary_logloss: 0.135691\n",
      "[63]\ttraining's auc: 0.929107\ttraining's binary_logloss: 0.105264\tvalid_1's auc: 0.831062\tvalid_1's binary_logloss: 0.135744\n",
      "[64]\ttraining's auc: 0.929375\ttraining's binary_logloss: 0.105092\tvalid_1's auc: 0.831075\tvalid_1's binary_logloss: 0.13574\n",
      "[65]\ttraining's auc: 0.930193\ttraining's binary_logloss: 0.10465\tvalid_1's auc: 0.831038\tvalid_1's binary_logloss: 0.135768\n",
      "[66]\ttraining's auc: 0.931248\ttraining's binary_logloss: 0.104246\tvalid_1's auc: 0.831066\tvalid_1's binary_logloss: 0.135749\n",
      "[67]\ttraining's auc: 0.931604\ttraining's binary_logloss: 0.103979\tvalid_1's auc: 0.831102\tvalid_1's binary_logloss: 0.135753\n",
      "[68]\ttraining's auc: 0.931978\ttraining's binary_logloss: 0.103738\tvalid_1's auc: 0.831387\tvalid_1's binary_logloss: 0.135687\n",
      "[69]\ttraining's auc: 0.932256\ttraining's binary_logloss: 0.103534\tvalid_1's auc: 0.83118\tvalid_1's binary_logloss: 0.135738\n",
      "[70]\ttraining's auc: 0.932531\ttraining's binary_logloss: 0.103362\tvalid_1's auc: 0.830981\tvalid_1's binary_logloss: 0.135783\n",
      "[71]\ttraining's auc: 0.932726\ttraining's binary_logloss: 0.103201\tvalid_1's auc: 0.830982\tvalid_1's binary_logloss: 0.135807\n",
      "[72]\ttraining's auc: 0.933189\ttraining's binary_logloss: 0.102914\tvalid_1's auc: 0.831259\tvalid_1's binary_logloss: 0.135758\n",
      "[73]\ttraining's auc: 0.933829\ttraining's binary_logloss: 0.102571\tvalid_1's auc: 0.831294\tvalid_1's binary_logloss: 0.135797\n",
      "[74]\ttraining's auc: 0.934011\ttraining's binary_logloss: 0.102426\tvalid_1's auc: 0.831186\tvalid_1's binary_logloss: 0.135834\n",
      "[75]\ttraining's auc: 0.934553\ttraining's binary_logloss: 0.102188\tvalid_1's auc: 0.831099\tvalid_1's binary_logloss: 0.135854\n",
      "[76]\ttraining's auc: 0.934788\ttraining's binary_logloss: 0.102011\tvalid_1's auc: 0.830847\tvalid_1's binary_logloss: 0.135913\n",
      "[77]\ttraining's auc: 0.936021\ttraining's binary_logloss: 0.101612\tvalid_1's auc: 0.831032\tvalid_1's binary_logloss: 0.135871\n",
      "[78]\ttraining's auc: 0.936172\ttraining's binary_logloss: 0.101481\tvalid_1's auc: 0.831214\tvalid_1's binary_logloss: 0.13584\n",
      "[79]\ttraining's auc: 0.936896\ttraining's binary_logloss: 0.101126\tvalid_1's auc: 0.830984\tvalid_1's binary_logloss: 0.135896\n",
      "[80]\ttraining's auc: 0.937047\ttraining's binary_logloss: 0.100987\tvalid_1's auc: 0.830957\tvalid_1's binary_logloss: 0.135904\n",
      "[81]\ttraining's auc: 0.937449\ttraining's binary_logloss: 0.100712\tvalid_1's auc: 0.831\tvalid_1's binary_logloss: 0.135878\n",
      "[82]\ttraining's auc: 0.937867\ttraining's binary_logloss: 0.100499\tvalid_1's auc: 0.830968\tvalid_1's binary_logloss: 0.135902\n",
      "[83]\ttraining's auc: 0.938042\ttraining's binary_logloss: 0.100334\tvalid_1's auc: 0.830923\tvalid_1's binary_logloss: 0.135924\n",
      "[84]\ttraining's auc: 0.938394\ttraining's binary_logloss: 0.10011\tvalid_1's auc: 0.83093\tvalid_1's binary_logloss: 0.135956\n",
      "[85]\ttraining's auc: 0.938586\ttraining's binary_logloss: 0.0999388\tvalid_1's auc: 0.830826\tvalid_1's binary_logloss: 0.135992\n",
      "[86]\ttraining's auc: 0.939012\ttraining's binary_logloss: 0.0997079\tvalid_1's auc: 0.830717\tvalid_1's binary_logloss: 0.136039\n",
      "[87]\ttraining's auc: 0.939698\ttraining's binary_logloss: 0.0994901\tvalid_1's auc: 0.830703\tvalid_1's binary_logloss: 0.136041\n",
      "[88]\ttraining's auc: 0.939889\ttraining's binary_logloss: 0.0993387\tvalid_1's auc: 0.830668\tvalid_1's binary_logloss: 0.136069\n",
      "[89]\ttraining's auc: 0.9404\ttraining's binary_logloss: 0.099068\tvalid_1's auc: 0.830439\tvalid_1's binary_logloss: 0.136126\n",
      "[90]\ttraining's auc: 0.94114\ttraining's binary_logloss: 0.0988412\tvalid_1's auc: 0.830399\tvalid_1's binary_logloss: 0.136137\n",
      "[91]\ttraining's auc: 0.941322\ttraining's binary_logloss: 0.0986809\tvalid_1's auc: 0.830233\tvalid_1's binary_logloss: 0.136188\n",
      "[92]\ttraining's auc: 0.941422\ttraining's binary_logloss: 0.0985537\tvalid_1's auc: 0.83016\tvalid_1's binary_logloss: 0.136218\n",
      "[93]\ttraining's auc: 0.941928\ttraining's binary_logloss: 0.0983874\tvalid_1's auc: 0.829935\tvalid_1's binary_logloss: 0.136278\n",
      "[94]\ttraining's auc: 0.942144\ttraining's binary_logloss: 0.0982117\tvalid_1's auc: 0.830075\tvalid_1's binary_logloss: 0.13626\n",
      "[95]\ttraining's auc: 0.942535\ttraining's binary_logloss: 0.0979328\tvalid_1's auc: 0.830329\tvalid_1's binary_logloss: 0.136216\n",
      "[96]\ttraining's auc: 0.943445\ttraining's binary_logloss: 0.0975723\tvalid_1's auc: 0.830693\tvalid_1's binary_logloss: 0.136167\n",
      "[97]\ttraining's auc: 0.94359\ttraining's binary_logloss: 0.0974372\tvalid_1's auc: 0.830749\tvalid_1's binary_logloss: 0.136186\n",
      "[98]\ttraining's auc: 0.94396\ttraining's binary_logloss: 0.0972384\tvalid_1's auc: 0.830526\tvalid_1's binary_logloss: 0.13624\n",
      "[99]\ttraining's auc: 0.944098\ttraining's binary_logloss: 0.0971126\tvalid_1's auc: 0.830586\tvalid_1's binary_logloss: 0.13627\n",
      "[100]\ttraining's auc: 0.94431\ttraining's binary_logloss: 0.0969278\tvalid_1's auc: 0.830603\tvalid_1's binary_logloss: 0.136261\n",
      "[101]\ttraining's auc: 0.944595\ttraining's binary_logloss: 0.0967626\tvalid_1's auc: 0.830584\tvalid_1's binary_logloss: 0.13625\n",
      "[102]\ttraining's auc: 0.945427\ttraining's binary_logloss: 0.0964574\tvalid_1's auc: 0.83044\tvalid_1's binary_logloss: 0.136318\n",
      "[103]\ttraining's auc: 0.945786\ttraining's binary_logloss: 0.096234\tvalid_1's auc: 0.830237\tvalid_1's binary_logloss: 0.136392\n",
      "[104]\ttraining's auc: 0.946176\ttraining's binary_logloss: 0.0960081\tvalid_1's auc: 0.830137\tvalid_1's binary_logloss: 0.136426\n",
      "[105]\ttraining's auc: 0.946827\ttraining's binary_logloss: 0.0956714\tvalid_1's auc: 0.830092\tvalid_1's binary_logloss: 0.136443\n",
      "[106]\ttraining's auc: 0.94742\ttraining's binary_logloss: 0.0953317\tvalid_1's auc: 0.830014\tvalid_1's binary_logloss: 0.136527\n",
      "[107]\ttraining's auc: 0.947784\ttraining's binary_logloss: 0.0951541\tvalid_1's auc: 0.829945\tvalid_1's binary_logloss: 0.136555\n",
      "[108]\ttraining's auc: 0.948041\ttraining's binary_logloss: 0.0949306\tvalid_1's auc: 0.830143\tvalid_1's binary_logloss: 0.136548\n",
      "[109]\ttraining's auc: 0.948326\ttraining's binary_logloss: 0.0946991\tvalid_1's auc: 0.83011\tvalid_1's binary_logloss: 0.136552\n",
      "[110]\ttraining's auc: 0.948473\ttraining's binary_logloss: 0.0945405\tvalid_1's auc: 0.830026\tvalid_1's binary_logloss: 0.136554\n",
      "[111]\ttraining's auc: 0.948681\ttraining's binary_logloss: 0.0943757\tvalid_1's auc: 0.830049\tvalid_1's binary_logloss: 0.136553\n",
      "[112]\ttraining's auc: 0.949039\ttraining's binary_logloss: 0.0941466\tvalid_1's auc: 0.830021\tvalid_1's binary_logloss: 0.136583\n",
      "[113]\ttraining's auc: 0.949488\ttraining's binary_logloss: 0.0939227\tvalid_1's auc: 0.830078\tvalid_1's binary_logloss: 0.13657\n",
      "[114]\ttraining's auc: 0.949571\ttraining's binary_logloss: 0.0937963\tvalid_1's auc: 0.83002\tvalid_1's binary_logloss: 0.136597\n",
      "[115]\ttraining's auc: 0.949659\ttraining's binary_logloss: 0.0936962\tvalid_1's auc: 0.830019\tvalid_1's binary_logloss: 0.13661\n",
      "[116]\ttraining's auc: 0.949947\ttraining's binary_logloss: 0.093503\tvalid_1's auc: 0.830095\tvalid_1's binary_logloss: 0.136579\n",
      "[117]\ttraining's auc: 0.950006\ttraining's binary_logloss: 0.0933958\tvalid_1's auc: 0.83015\tvalid_1's binary_logloss: 0.136594\n",
      "[118]\ttraining's auc: 0.950496\ttraining's binary_logloss: 0.0931176\tvalid_1's auc: 0.829998\tvalid_1's binary_logloss: 0.136644\n",
      "[119]\ttraining's auc: 0.950718\ttraining's binary_logloss: 0.0929487\tvalid_1's auc: 0.829911\tvalid_1's binary_logloss: 0.136679\n",
      "[120]\ttraining's auc: 0.951164\ttraining's binary_logloss: 0.09271\tvalid_1's auc: 0.829637\tvalid_1's binary_logloss: 0.136802\n",
      "[121]\ttraining's auc: 0.951471\ttraining's binary_logloss: 0.0925528\tvalid_1's auc: 0.829552\tvalid_1's binary_logloss: 0.136824\n",
      "[122]\ttraining's auc: 0.951821\ttraining's binary_logloss: 0.0922922\tvalid_1's auc: 0.829511\tvalid_1's binary_logloss: 0.136832\n",
      "[123]\ttraining's auc: 0.951979\ttraining's binary_logloss: 0.0921593\tvalid_1's auc: 0.829561\tvalid_1's binary_logloss: 0.136838\n",
      "[124]\ttraining's auc: 0.952824\ttraining's binary_logloss: 0.0918067\tvalid_1's auc: 0.829444\tvalid_1's binary_logloss: 0.13692\n",
      "[125]\ttraining's auc: 0.952926\ttraining's binary_logloss: 0.0916801\tvalid_1's auc: 0.82949\tvalid_1's binary_logloss: 0.136923\n",
      "[126]\ttraining's auc: 0.953078\ttraining's binary_logloss: 0.0915494\tvalid_1's auc: 0.829465\tvalid_1's binary_logloss: 0.136942\n",
      "[127]\ttraining's auc: 0.95348\ttraining's binary_logloss: 0.0913052\tvalid_1's auc: 0.829413\tvalid_1's binary_logloss: 0.136959\n",
      "[128]\ttraining's auc: 0.953652\ttraining's binary_logloss: 0.0911477\tvalid_1's auc: 0.829357\tvalid_1's binary_logloss: 0.136967\n",
      "[129]\ttraining's auc: 0.953902\ttraining's binary_logloss: 0.0910037\tvalid_1's auc: 0.829564\tvalid_1's binary_logloss: 0.136957\n",
      "[130]\ttraining's auc: 0.954299\ttraining's binary_logloss: 0.090782\tvalid_1's auc: 0.829621\tvalid_1's binary_logloss: 0.136961\n",
      "[131]\ttraining's auc: 0.954764\ttraining's binary_logloss: 0.0905679\tvalid_1's auc: 0.829477\tvalid_1's binary_logloss: 0.137027\n",
      "[132]\ttraining's auc: 0.955036\ttraining's binary_logloss: 0.090362\tvalid_1's auc: 0.829495\tvalid_1's binary_logloss: 0.137065\n",
      "[133]\ttraining's auc: 0.955984\ttraining's binary_logloss: 0.0900336\tvalid_1's auc: 0.829275\tvalid_1's binary_logloss: 0.137156\n",
      "[134]\ttraining's auc: 0.956325\ttraining's binary_logloss: 0.0897815\tvalid_1's auc: 0.829209\tvalid_1's binary_logloss: 0.137213\n",
      "[135]\ttraining's auc: 0.956791\ttraining's binary_logloss: 0.0895153\tvalid_1's auc: 0.829296\tvalid_1's binary_logloss: 0.137204\n",
      "[136]\ttraining's auc: 0.957063\ttraining's binary_logloss: 0.0893107\tvalid_1's auc: 0.829182\tvalid_1's binary_logloss: 0.13726\n",
      "[137]\ttraining's auc: 0.957139\ttraining's binary_logloss: 0.0892199\tvalid_1's auc: 0.829225\tvalid_1's binary_logloss: 0.137283\n",
      "[138]\ttraining's auc: 0.957703\ttraining's binary_logloss: 0.0889509\tvalid_1's auc: 0.829346\tvalid_1's binary_logloss: 0.137301\n",
      "[139]\ttraining's auc: 0.957858\ttraining's binary_logloss: 0.0887939\tvalid_1's auc: 0.829285\tvalid_1's binary_logloss: 0.137346\n",
      "[140]\ttraining's auc: 0.957928\ttraining's binary_logloss: 0.0886885\tvalid_1's auc: 0.829251\tvalid_1's binary_logloss: 0.137367\n",
      "[141]\ttraining's auc: 0.958002\ttraining's binary_logloss: 0.0885732\tvalid_1's auc: 0.829082\tvalid_1's binary_logloss: 0.137412\n",
      "[142]\ttraining's auc: 0.958178\ttraining's binary_logloss: 0.0884055\tvalid_1's auc: 0.82902\tvalid_1's binary_logloss: 0.137438\n",
      "Early stopping, best iteration is:\n",
      "[42]\ttraining's auc: 0.914751\ttraining's binary_logloss: 0.111472\tvalid_1's auc: 0.831334\tvalid_1's binary_logloss: 0.135504\n",
      "ROC AUC:  0.8372\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "\n",
    "eval_set = [(X_tr, y_tr),(X_val, y_val)]\n",
    "lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric='auc', eval_set=eval_set)\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC: {0: .4f}'.format(lgbm_roc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_search_space={\n",
    "    'num_leaves': hp.quniform('num_leaves', 32, 64, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 100, 160, 1),\n",
    "    'min_child_samples': \n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c65fd6502b3330fa03fc67e76fa9fe1b03f85e990f5924fb7f950a8faecf01bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
