{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "data_df = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 로지스틱 회귀와 KNN을 기반으로 소프트 보팅 방식으로 새롭게 보팅 분류기 만듦\n",
    "- VotingClassifier 클래스 : 보팅 분류기 생성\n",
    "- 매개변수: estimators(리스트 값으로 보팅에 사용될 여러개의 Classifier 객체들을 튜플형식으로)와 voting('hard'시 하드보팅, 'soft'시 소프트 보팅)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#개별 모델은 로지스틱 회귀와 KNN\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=8)\n",
    "\n",
    "#개별 모델을 소프트 보팅을 기반의 앙상블 모델로 구현한 분류기\n",
    "vo_clf = VotingClassifier(estimators=[('LR',lr_clf),('KNN',knn_clf)], voting='soft')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target, test_size=0.2, random_state=156)\n",
    "\n",
    "#학습, 예측, 평가\n",
    "vo_clf.fit(X_train, y_train)\n",
    "pred = vo_clf.predict(X_test)\n",
    "print('Voting 분류기 정확도: {0: .4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 개별 모델의 학습/예측/평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = [lr_clf, knn_clf]\n",
    "for classifier in classifiers:\n",
    "    classifier.fit(X_train, y_train)\n",
    "    pred = classifier.predict(X_test)\n",
    "    class_name=classifier.__class__.__name__\n",
    "    print('{0}정확도: {1: .4f}'.format(class_name, accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 랜덤포레스트(배깅에 속함!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_new_feature_name_df(old_feature_name_df):\n",
    "    feature_dup_df = pd.DataFrame(data=old_feature_name_df.groupby('column_name').cumcount(), columns=['dup_cnt'])\n",
    "    feature_dup_df = feature_dup_df.reset_index()\n",
    "    new_feature_name_df = pd.merge(old_feature_name_df.reset_index(), feature_dup_df, how='outer')\n",
    "    new_feature_name_df['column_name']=new_feature_name_df[['column_name','dup_cnt']].apply(lambda x:x[0]+'_'+str(x[1]) if x[1]>0 else x[0],axis=1)\n",
    "    new_feature_name_df = new_feature_name_df.drop(['index'],axis=1)\n",
    "    return new_feature_name_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_human_dataset():\n",
    "    feature_name_df = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/features.txt', sep='\\s+', header=None, names=['column_index','column_name'])\n",
    "    #중복된 피처명을 수정하는 get_new_feature_name_df()을 이용, 신규 피처명 Dataframe 생성\n",
    "    new_feature_name_df = get_new_feature_name_df(feature_name_df)\n",
    "    #DataFrame에 피처명을 칼럼으로 부여하기 위해 리스트 객체로 다시 반환\n",
    "    feature_name = new_feature_name_df.iloc[:,1].values.tolist()\n",
    "    #학습 피처 데이터 세트와 테스트 피처 데이터를 DataFrame으로 로딩, 칼럼명은 feature_name 적용\n",
    "    X_train = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/train/X_train.txt', sep='\\s+', header=None, names=feature_name)\n",
    "    X_test = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/test/X_test.txt', sep='\\s+', header=None, names=feature_name)\n",
    "\n",
    "    y_train = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/train/y_train.txt', sep='\\s+', header=None, names=['action'])\n",
    "    y_test = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/human_activity/test/y_test.txt', sep='\\s+', header=None, names=['action'])\n",
    "\n",
    "    return X_train,X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "#랜덤 포스 학습 및 별도의 테트 세트 예측성능 평가\n",
    "rf_clf = RandomForestClassifier(random_state=0, max_depth=8)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "pred = rf_clf.predict(X_test)\n",
    "accuracy= accuracy_score(y_test, pred)\n",
    "print('랜덤 포레스트 정확도: {0: .4f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = {\n",
    "    'max_depth': [8,16,24],\n",
    "    'min_samples_leaf':[1,6,12],\n",
    "    'min_samples_split':[2,8,16]\n",
    "}\n",
    "\n",
    "#RandomForestClassifier 객체 생성 후 GridSearchCV 수행\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0, n_jobs=-1)\n",
    "grid_cv = GridSearchCV(rf_clf, param_grid=params, cv=2, n_jobs=-1)\n",
    "grid_cv.fit(X_train, y_train)\n",
    "print('최적 하이퍼 파라미터:\\n', grid_cv.best_params_)\n",
    "print('최고 예측 정확도: {0: .4f}'.format(grid_cv.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최적의 파라미터로 학습, 예측 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_clf1 = RandomForestClassifier(n_estimators=100, min_samples_leaf=6, max_depth=16, min_samples_split=2, random_state=0)\n",
    "rf_clf1.fit(X_train, y_train)\n",
    "rf_clf1.predict(X_test)\n",
    "print('예측 정확도: {0: .4f}'.format(accuracy_score(y_test, pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "ftr_importances_values = rf_clf1.feature_importances_\n",
    "ftr_importances = pd.Series(ftr_importances_values, index=X_train.columns)\n",
    "ftr_top20 = ftr_importances.sort_values(ascending=False)[:20]\n",
    "\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.title('Feature importances Top 20')\n",
    "sns.barplot(x=ftr_top20, y=ftr_top20.index)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GBM(부스팅 알고리즘) 기반 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_human_dataset()\n",
    "\n",
    "#GBM 수행시간 측정을 위함, 시작시간 설정\n",
    "start_time = time.time()\n",
    "gb_clf = GradientBoostingClassifier(random_state=0)\n",
    "gb_clf.fit(X_train, y_train)\n",
    "gb_pred = gb_clf.predict(X_test)\n",
    "gb_accuracy = accuracy_score(y_test, pred)\n",
    "\n",
    "print('GBM 정확도: {0: .4f}'.format(gb_accuracy))\n",
    "print('GBM 수행 시간: {0: .1f} 초'.format(time.time()-start_time))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tkinter.ttk import LabeledScale\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "features = dataset.data\n",
    "labels = dataset.target\n",
    "\n",
    "cancer_df = pd.DataFrame(data=features, columns=dataset.feature_names)\n",
    "cancer_df['target']=labels\n",
    "cancer_df.head(3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.target_names)\n",
    "print(cancer_df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features = cancer_df.iloc[:, :-1]\n",
    "y_label = cancer_df.iloc[:, -1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_label, test_size=0.2, random_state=156)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=156)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(X_tr.shape, X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtr = xgb.DMatrix(data=X_tr, label=y_tr)\n",
    "dval = xgb.DMatrix(data=X_val, label=y_val)\n",
    "dtest = xgb.DMatrix(data=X_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth':3,\n",
    "    'eta':0.05,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'logloss'\n",
    "}\n",
    "num_rounds= 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_list = [(dtr, 'train'), (dval, 'eval')]\n",
    "xgb_model = xgb.train(params=params, dtrain=dtr, num_boost_round = num_rounds, early_stopping_rounds=50, evals=eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probs=xgb_model.predict(dtest)\n",
    "print('predict() 수행 결과값을 10개만 표시, 예측 확률값으로 표시됨')\n",
    "print(np.round(pred_probs[:10],3))\n",
    "\n",
    "preds=[1 if x>0.5 else 0 for x in pred_probs]\n",
    "print('예측값 10개만 표시', preds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred=None, pred_proba=None):\n",
    "    \n",
    "    confusion = confusion_matrix(y_test, pred)\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred)\n",
    "    recall = recall_score(y_test, pred)\n",
    "    f1 = f1_score(y_test, pred)\n",
    "    roc_auc = roc_auc_score(y_test, pred_proba)\n",
    "    print('오차행렬')\n",
    "    print(confusion)\n",
    "    print('정확도: {0:.4f}, 정밀도:{1: .4f}, 재현율:{2: .4f}, f1점수: {3: .4f}, AUC점수:{4: .4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, preds, pred_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 사이킷런 래퍼 XGBClassifier, XGBRegressor\n",
    "- 매개변수: eta -> learning_rate, sub_sample->subsample, lambda->reg_lambda, alpha->reg_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3, eval_metric='logloss')\n",
    "xgb_wrapper.fit(X_train, y_train, verbose=True)\n",
    "w_preds = xgb_wrapper.predict(X_test)\n",
    "w_pred_proba=xgb_wrapper.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, w_preds, w_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조기중단 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "from xgboost import XGBClassifier\n",
    "xgb_wrapper = XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3)\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "ws50_preds = xgb_wrapper.predict(X_test)\n",
    "ws50_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, ws50_preds, ws50_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 조기종료 라운드 수 10으로 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_wrapper.fit(X_tr, y_tr, early_stopping_rounds=10, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "ws10_pred = xgb_wrapper.predict(X_test)\n",
    "ws10_pred_proba = xgb_wrapper.predict_proba(X_test)[:, 1]\n",
    "get_clf_eval(y_test, ws10_pred, ws10_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = load_breast_cancer()\n",
    "cancer_df = pd.DataFrame(data=dataset.data, columns=dataset.feature_names)\n",
    "cancer_df['target'] = dataset.target\n",
    "X_feature = cancer_df.iloc[:,:-1]\n",
    "y_label = cancer_df.iloc[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feature, y_label, test_size=0.2, random_state=156)\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=156)\n",
    "\n",
    "lgbm_wrapper = LGBMClassifier(n_estimators=400, learning_rate=0.05)\n",
    "\n",
    "evals = [(X_tr, y_tr), (X_val, y_val)]\n",
    "lgbm_wrapper.fit(X_tr, y_tr, early_stopping_rounds=50, eval_metric='logloss', eval_set=evals, verbose=True)\n",
    "preds = lgbm_wrapper.predict(X_test)\n",
    "pred_proba = lgbm_wrapper.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_clf_eval(y_test, preds, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,12))\n",
    "plot_importance(lgbm_wrapper, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 베이지안 최적화 기반의 HyperOpt를 이용한 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'max_depth' : [10,20,30,40,50], \n",
    "    'num_leaves' : [35,45,55,65],\n",
    "    'colsample_bytree':[0.5, 0.6,0.7,0.8,0.9], \n",
    "    'subsample': [0.5, 0.6, 0.7, 0.8, 0.9],\n",
    "    'min_child_weight': [10,20,30,40],\n",
    "    'reg_alpha': [0.01, 0.05, 0.1]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 분류 실습 - 캐들 산탄데르 고객 만족 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "cust_df = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/train.csv', encoding='latin1')\n",
    "print('dataset shape', cust_df.shape)\n",
    "cust_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cust_df['TARGET'].value_counts())\n",
    "unsatisfied_cnt = cust_df[cust_df['TARGET']==1].TARGET.count()\n",
    "total_cnt = cust_df.TARGET.count()\n",
    "print('unsatisfied 비율은 {0: .2f}'.format(unsatisfied_cnt/total_cnt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "var3의 평균값 -999999 -> NaN이나 특정 예외값을 -999999fh 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cust_df.var3.value_counts()[:10]) #-999999값이 116개 있음 -> 다른 값에 비해 편차가 심하므로 2로 뱐환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df['var3'].replace(-999999, 2, inplace=True)\n",
    "#cust_df.drop('ID', axis=1, inplace=True)\n",
    "\n",
    "#피처 세트와 레이블 분리, 레이블 칼럼은 DataFrame 마지막에 위치해 -1로 분리\n",
    "X_feature = cust_df.iloc[:,:-1]\n",
    "y_labels = cust_df.iloc[:, -1]\n",
    "print('피처 데이터 shape:{0}'.format(X_feature.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_feature, y_labels, test_size=0.2, random_state=0)\n",
    "\n",
    "train_cnt = y_train.count()\n",
    "test_cnt = y_test.count()\n",
    "\n",
    "print('학습 세트 Shape: {0}, 테스트 세트 shape:{1}'.format(X_train.shape, X_test.shape))\n",
    "print('학습 세트 레이블 값 분포 비율')\n",
    "print(y_train.value_counts()/train_cnt)\n",
    "print('\\n 테스트 세트 레이블 값 분포 비율')\n",
    "print(y_test.value_counts()/test_cnt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost 조기 중단의 검증 데이터 세트로 사용하기 위해 학습 데이터 쪼개기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- XGBoost 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#n_estimator는 500으로, random_state는 예제 수행 시마다 동일 예측 결과를 위해 수정\n",
    "xgb_clf = XGBClassifier(n_estimator=500, learning_rate=0.05, random_state=156)\n",
    "\n",
    "#성능 평가 지표는 auc로 조기 중간 파라밈터는 100으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=[(X_tr, y_tr), (X_val, y_val)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC: {0: .4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp\n",
    "#max_depth는 5에서 15까지 1간격으로, min_child_weight는 1~6까지 1간격으로\n",
    "#colsample_bytree는 0.5에서 0.95사이, learning_rate는 0.01dptj 0.2사이 정규 분포된 값으로 검색\n",
    "#quniform : 간격설정, uniform: 사이\n",
    "xgb_search_space = {\n",
    "    'max_depth': hp.quniform('max_depth',5,15,1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1,6,1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 2)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def objective_func(search_space):\n",
    "    xgb_clf = XGBClassifier(n_estimator=100, \n",
    "    max_depth=int(search_space['max_depth']), \n",
    "    min_child_weight=int(search_space['min_child_weight']), \n",
    "    colsample_bytree=search_space['colsample_bytree'], \n",
    "    learning_rate=search_space['learning_rate'])\n",
    "\n",
    "    #3개의 k-fold 방식으로 평가된 roc-auc 지표를 담는 리스트\n",
    "    roc_auc_list = []\n",
    "\n",
    "    #3개의 k-fold방식 적용\n",
    "    kf = KFold(n_splits=3)\n",
    "    #X_train을 다시 학습과 검증 데이터로 분리\n",
    "    for tr_index, val_index in kf.split(X_train):\n",
    "        #kf.split(X_train)으로 추출된 학습과 검증 index값으로 학습과 검증 데이터 분리\n",
    "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
    "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "\n",
    "        xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=30, eval_metric='auc', eval_set=[(X_tr, y_tr), (X_val, y_val)])\n",
    "\n",
    "        score = roc_auc_score(y_val, xgb_clf.predict_proba(X_val)[:,1])\n",
    "        roc_auc_list.append(score)\n",
    "\n",
    "    #3개의 k-fold로 계산된 roc_auc값의 평균값을 반환하되\n",
    "    #HyperOpt는 목적 함수의 최소값을 위한 입력값을 찾음으로 -1을 곱한 뒤 반환\n",
    "    return -1*np.mean(roc_auc_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from hyperopt import fmin, tpe, Trials\n",
    "import numpy as np\n",
    "trials = Trials()\n",
    "xgb_search_space = {\n",
    "    'max_depth': hp.quniform('max_depth',5,15,1),\n",
    "    'min_child_weight': hp.quniform('min_child_weight', 1,6,1),\n",
    "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 0.95),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 2)\n",
    "}\n",
    "\n",
    "#fmin()함수 호출, max_evals지정된 횟수만큼 반복 후 목적 함수 최소값을 가지는 최적 입력값 추출\n",
    "best = fmin(fn=objective_func, space=xgb_search_space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=np.random.default_rng(seed=30))\n",
    "\n",
    "# print('best:',best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_estimators를 500 증가 후 최적으로 찾은 하이퍼 파라미터를 기반으로 학습과 예측 수행\n",
    "xgb_clf = XGBClassifier(n_estimators=500, learning_rate=round(best['learning_rate'],5), max_depth=int(best['max_depth']),\n",
    "min_child_weight=int(best['min_child_weight']), colsample_bytree=round(best['colsample_bytree'],15))\n",
    "\n",
    "#evaluation metric을 auc로, early stopping은 100으로 설정하고 학습 수행\n",
    "xgb_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric='auc', eval_set=[(X_tr, y_tr),(X_val, y_val)])\n",
    "\n",
    "xgb_roc_score = roc_auc_score(y_test, xgb_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC 점수: {0: .4f}'.format(xgb_roc_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import plot_importance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig,ax = plt.subplot(1,1,figsize=(10,8))\n",
    "plot_importance(xgb_clf, ax=ax, max_num_features=20, height=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LightGBM 모델 학습과 하이퍼 파라미터 튜닝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "\n",
    "eval_set = [(X_tr, y_tr),(X_val, y_val)]\n",
    "lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds=100, eval_metric='auc', eval_set=eval_set)\n",
    "\n",
    "lgbm_roc_score = roc_auc_score(y_test, lgbm_clf.predict_proba(X_test)[:,1])\n",
    "print('ROC AUC: {0: .4f}'.format(lgbm_roc_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_search_space={\n",
    "    'num_leaves': hp.quniform('num_leaves', 32, 64, 1),\n",
    "    'max_depth': hp.quniform('max_depth', 100, 160, 1),\n",
    "    'min_child_samples': hp.quniform('min_child_samples', 60, 100,1),\n",
    "    'subsample':hp.uniform('subsample', 0.7,1),\n",
    "    'learning_rate': hp.uniform('learning_rate', 0.01, 0.2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_func(search_space):\n",
    "\n",
    "    lgbm_clf = LGBMClassifier(n_estimators=100, \n",
    "    num_leaves=int(search_space['num_leaves']), \n",
    "    max_depth=int(search_space['max_depth']), \n",
    "    min_child_samples=int(search_space['min_child_samples']), \n",
    "    subsample=search_space['subsample'], \n",
    "    learning_rate=search_space['learning_rate'])\n",
    "\n",
    "    #3개 k-fold방식으로 평가된 roc-auc 지표를 담는 list\n",
    "    roc_auc_list=[]\n",
    "    kf=KFold(n_splits=3)\n",
    "    for tr_index, val_index in kf.split(X_train):\n",
    "        X_tr, y_tr = X_train.iloc[tr_index], y_train.iloc[tr_index]\n",
    "        X_val, y_val = X_train.iloc[val_index], y_train.iloc[val_index]\n",
    "        lgbm_clf.fit(X_tr, y_tr, early_stopping_rounds=30, eval_metric='auc', eval_set=[(X_tr, y_tr),(X_val, y_val)])\n",
    "        score = roc_auc_score(y_val, lgbm_clf.predict_proba(X_val)[:,1])\n",
    "        roc_auc_list.append(score)\n",
    "\n",
    "    return -1*np.mean(roc_auc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, Trials\n",
    "\n",
    "trials = Trials()\n",
    "\n",
    "best =fmin(fn=objective_func, space=lgbm_search_space, algo=tpe.suggest, max_evals=50, trials=trials, rstate=np.random.default_rng(seed=30))\n",
    "\n",
    "print('best:', best)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 캐글 신용카드 사기 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline\n",
    "\n",
    "card_df = pd.read_csv('/Users/stillssi/Desktop/MLP-Python/실습/실습 채우기/머신러닝/datas/creditcard.csv')\n",
    "card_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "card_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy=df.copy()\n",
    "    df_copy.drop('Time', axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset(df=None):\n",
    "    df_copy=get_preprocessed_df(df)\n",
    "    X_features= df_copy.iloc[:, :-1]\n",
    "    y_target = df_copy.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('학습 데이터 레이블 값 비율')\n",
    "print(y_train.value_counts()/y_train.count())\n",
    "print('테스트 데이터 비율')\n",
    "print(y_test.value_counts()/y_test.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_pred = lr_clf.predict(X_test)\n",
    "lr_pred_proba = lr_clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "get_clf_eval(y_test, lr_pred, lr_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lightGBM 이용한 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
    "    model.fit(ftr_train, tgt_train)\n",
    "    pred = model.predict(ftr_test)\n",
    "    pred_proba = model.predict_proba(ftr_test)[:,1]\n",
    "    get_clf_eval(tgt_test, pred, pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.xticks(range(0,30000,1000), rotation=60)\n",
    "sns.histplot(card_df['Amount'], bins=100, kde=True)\n",
    "plt.show()\n",
    "#1000불 이하인 데이터가 대부분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Amount를 표준 정규 분포 형태로 변환한 뒤 로지스틱 회귀의 예측 성능을 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy=df.copy()\n",
    "    scaler = StandardScaler()\n",
    "    amount_n = scaler.fit_transform(df_copy['Amount'].values.reshape(-1,1))\n",
    "    df_copy.insert(0,'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "print('###로지스틱 회귀 예측 성능###')\n",
    "lr_clf = LogisticRegression(max_iter=1000)\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('##LightGBM 예측 성능##')\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StandScaler가 아닌 로그 변환 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_df(df=None):\n",
    "    df_copy= df.copy()\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "print('###로지스틱 회귀 예측 성능###')\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('###LightGBM 예측 성능###')\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(9,9))\n",
    "corr = card_df.corr()\n",
    "sns.heatmap(corr, cmap='RdBu') #Rdbu: 양의 상관관계가 높을 수록 진한 파랑, 음의 상관관계일수록 빨간색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 결정 레이블인 Class와 음의 상관계수가 가장 높은 V14, V17 이상치 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    fraud=df[df['Class']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "\n",
    "    iqr = quantile_75 - quantile_25\n",
    "    iqr_weight = iqr*weight\n",
    "    lowest_val = quantile_25-iqr_weight\n",
    "    highest_val=quantile_75+iqr_weight\n",
    "\n",
    "    outlier_index = fraud[(fraud<lowest_val)|(fraud>highest_val)].index\n",
    "    return outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)\n",
    "print('이상치 데이터 인덱스:', outlier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_df(df=None):\n",
    "    df_copy= df.copy()\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time', 'Amount'], axis=1, inplace=True)\n",
    "    #이상치 데이터 삭제하는 로직 추가\n",
    "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
    "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "    return df_copy\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)\n",
    "print('###로지스틱 회귀 예측 성능###')\n",
    "get_model_train_eval(lr_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "\n",
    "print('###LightGBM 예측 성능###')\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 기본 스태킹 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "cancer_data = load_breast_cancer()\n",
    "\n",
    "X_data = cancer_data.data\n",
    "y_label = cancer_data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_label, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf = KNeighborsClassifier(n_neighbors=4)\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier(n_estimators=100)\n",
    "\n",
    "lr_final = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_clf.fit(X_train, y_train)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = knn_clf.predict(X_test)\n",
    "rf_pred = rf_clf.predict(X_test)\n",
    "dt_pred = dt_clf.predict(X_test)\n",
    "ada_pred = ada_clf.predict(X_test)\n",
    "\n",
    "print('KNN 정확도: {0: .4f}'.format(accuracy_score(y_test, knn_pred)))\n",
    "print('랜덤 포레스트 정확도: {0: .4f}'.format(accuracy_score(y_test,rf_pred)))\n",
    "print('결정 트리 정확도: {0: .4f}'.format(accuracy_score(y_test, dt_pred)))\n",
    "print('에이다 정확도: {0: .4f}'.format(accuracy_score(y_test, ada_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = np.array([knn_pred, rf_pred, dt_pred, ada_pred])\n",
    "print(pred.shape)\n",
    "\n",
    "pred = np.transpose(pred)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final.fit(pred, y_test)\n",
    "final = lr_final.predict(pred)\n",
    "\n",
    "print('최종 메타 모델의 예측 정확도: {0: .4f}'.format(accuracy_score(y_test, final)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV세트 기반의 스태킹\n",
    "- 스태킹 모델은 과적합을 개선하기 위해 최종 메타 모델을 위한 데이터 세트를 만들 때 교차 검증 기반으로 예측된 결과 데이터 세트 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n",
    "    #지정된 n_folds값으로 KFold 생성\n",
    "    kf= KFold(n_splits=n_folds, shuffle=False)\n",
    "    #추후에 메타 모델이 사용할 학습 데이터 반환을 위한 넘파이 배열 초기화\n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0], 1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0], n_folds))\n",
    "    print(model.__class__.__name__, 'model 시작')\n",
    "\n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        #입력된 학습 데이터에서 기반 모델이 학습/예측할 폴드 데이터 세트 추출\n",
    "        print('\\t 폴드 세트: ', folder_counter, '시작')\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index]\n",
    "\n",
    "        #폴드 세트 내부에서 다시 만들어진 학습 데이터를 기반 모델의 학습 수행\n",
    "        model.fit(X_tr, y_tr)  \n",
    "        #폴드 세트 내부에서 다시 만들어진 검증 데이터로 기반 모델 예측 후 데이터 저장\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1)\n",
    "        #입력된 원본 테스트 데이터를 폴드 세트 내 학습된 기반 모델에서 예측 후 데이터 저장\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n)\n",
    "\n",
    "    #폴드 세트 내에서 원본 테스트 데이터를 예측한 데이터를 평균하여 테스트 데이터로 생성\n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n",
    "    #train_fold_pred는 최종 메타 모델이 사용하는 학습 데이터, test_pred_mean은 테스트 데이터\n",
    "    return train_fold_pred, test_pred_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_train, knn_test = get_stacking_base_datasets(knn_clf, X_train, y_train, X_test, 7)\n",
    "rf_train, rf_test = get_stacking_base_datasets(rf_clf,X_train, y_train, X_test, 7)\n",
    "dt_train, dt_test = get_stacking_base_datasets(dt_clf, X_train, y_train, X_test, 7)\n",
    "ada_train, ada_test = get_stacking_base_datasets(ada_clf, X_train, y_train, X_test, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stack_final_X_train = np.concatenate((knn_train, rf_train, dt_train, ada_train), axis=1)\n",
    "Stack_final_X_test = np.concatenate((knn_test, rf_test, dt_test, ada_test), axis=1)\n",
    "print('원본 학습 피처 데이터 Shpae:', X_train.shape, '원본 테스트 피처 Shape:', X_test.shape)\n",
    "print('스태킹 학습 피처 데이터 Shape:', Stack_final_X_train.shape, '스태킹 테스트 피처 데이터 Shape:', Stack_final_X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_final.fit(Stack_final_X_train, y_train)\n",
    "stack_final = lr_final.predict(Stack_final_X_test)\n",
    "print('최종 메타 모델의 예측 정확도: {0: .4f}'.format(accuracy_score(y_test, stack_final)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c65fd6502b3330fa03fc67e76fa9fe1b03f85e990f5924fb7f950a8faecf01bf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
